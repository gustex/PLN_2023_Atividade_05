{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gustex/PLN_2023_Atividade_05/blob/main/PLN_Atividade_Pr%C3%A1tica_05_Gustavo_Teixeira.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2023.Q3]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m67OOx9MX_3"
      },
      "source": [
        "### **ATIVIDADE PRÁTICA 05 [LangChain + Grandes Modelos de Linguagem + API]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gk0nHKabBT-"
      },
      "source": [
        "A **ATIVIDADE PRÁTICA 05** deve ser feita utilizando o **Google Colab** com uma conta sua vinculada ao Gmail. O link do seu notebook, armazenado no Google Drive, além do link de um repositório no GitHub e os principais resultados da atividade, devem ser enviados usando o seguinte formulário:\n",
        "\n",
        "> https://forms.gle/C1oUi1FKTZ4W9fNdA\n",
        "\n",
        "\n",
        "**IMPORTANTE**: A submissão deve ser feita até o dia **12/12 (terça-feira)** APENAS POR UM INTEGRANTE DA EQUIPE, até às 23h59. Por favor, lembre-se de dar permissão de ACESSO IRRESTRITO para o professor da disciplina de PLN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7hJlilKM485"
      },
      "source": [
        "### **EQUIPE**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POR FAVOR, PREENCHER OS INTEGRANDES DA SUA EQUIPE:**\n",
        "\n",
        "\n",
        "**Integrante 01:**\n",
        "\n",
        "`Por favor, informe o seu nome completo e RA:`\n",
        "\n",
        "Gustavo da Silva Teixeira - RA 11201812255"
      ],
      "metadata": {
        "id": "tnIArN0QY-Ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GRANDE MODELO DE LINGUAGEM (*Large Language Model - LLM*)**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "VbYD2mw8y4CN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cada equipe deve selecionar um Grande Modelo de Linguagem (*Large Language Model - LMM*). Preferencialmente, usar um modelo gratuito. Cada modelo pode ser escolhido por até 4 equipes.\n",
        "\n",
        ">\n",
        "\n",
        "Uma lista de LLMs está disponível em:\n",
        "\n",
        "> https://integrations.langchain.com/llms\n",
        "> https://python.langchain.com/docs/integrations/llms/"
      ],
      "metadata": {
        "id": "_UlblxFxzDV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelos da Hugging Face:**\n",
        "\n",
        "* Mistral Base: modelo promissor.\n",
        "\n",
        "> https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\n",
        "\n",
        "* Mistral Quantizado: mais leve. Pode ser um pouco mais difícil de configurar, mas deve ocupar menos memória.\n",
        "\n",
        "> https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF\n",
        "\n",
        "* Mistral Lite da Amazon: pode ter desempenho melhor.\n",
        "\n",
        "> https://huggingface.co/amazon/MistralLite\n",
        "\n",
        "* Llama 2 7B: modelo da Meta melhorado.\n",
        "\n",
        "> https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
        "\n",
        "* Llama 2 7b 32k: contexto maior pelo mesmo tamanho.\n",
        "\n",
        "> https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct\n",
        "\n",
        "* Galactica: para artigos científicos\n",
        "\n",
        "> https://huggingface.co/facebook/galactica-6.7b\n",
        "\n",
        "* Alpaca: alternativo ao Llama\n",
        "\n",
        "> https://huggingface.co/chavinlo/alpaca-native\n",
        "\n",
        "> https://huggingface.co/spaces/tloen/alpaca-lora"
      ],
      "metadata": {
        "id": "GVpiVhzn3QqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lista de Modelos Interessantes:**\n",
        "\n",
        "> https://www.kdnuggets.com/2023/04/8-opensource-alternative-chatgpt-bard.html\n",
        "\n",
        "* Vicuna:\n",
        "> https://huggingface.co/lmsys/vicuna-7b-v1.5-16k\n",
        "\n",
        "* Openchat kit:\n",
        "> https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B\n",
        "\n",
        "* Meta OPT:\n",
        "> https://huggingface.co/facebook/opt-1.3b\n"
      ],
      "metadata": {
        "id": "WRuAiSr05Ayt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**APIs:**\n",
        "\n",
        "* GPT4ALL: tenta ser um chatGPT aberto\n",
        "> https://python.langchain.com/docs/integrations/llms/gpt4all\n",
        "\n",
        "* YandexGPT: da empresa russa que criou um modelo de *Machine Learning* clássico muito bom, o CatBoost.\n",
        "> https://python.langchain.com/docs/integrations/llms/yandex\n",
        "\n",
        "* Anthropic: empresa forte concorrente da OpenAi.\n",
        "> https://python.langchain.com/docs/integrations/chat/anthropic\n",
        "\n",
        "* Gorilla: pipeline para geração de código\n",
        " > https://www.kdnuggets.com/2023/06/meet-gorilla-uc-berkeley-microsoft-apiaugmented-llm-outperforms-gpt4-chatgpt-claude.html"
      ],
      "metadata": {
        "id": "5Q3mhqda5WP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE**: todo esse levantamento e comentários foram feitos pelo aluno  **Bruno Sanches Rodrigues**."
      ],
      "metadata": {
        "id": "DO6XMjHx58u9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por favor, informe os dados do LLM selecionada:\n",
        "\n",
        ">\n",
        "\n",
        "\n",
        "**LLM**:\n",
        "\n",
        ">Flan-T5\n",
        "\n",
        "**Link para a documentação oficial**:\n",
        "\n",
        ">https://huggingface.co/google/flan-t5-base\n",
        "\n",
        "\n",
        "**Site oficial (GitHub)**:\n",
        "\n",
        ">https://huggingface.co/google/flan-t5-base\n",
        "\n",
        "https://github.com/google-research/t5x"
      ],
      "metadata": {
        "id": "a6AkE6iW0c3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE**: não pode ser o modelo da `OpenAI`."
      ],
      "metadata": {
        "id": "A2oo8GtK0xSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **API**\n",
        "---"
      ],
      "metadata": {
        "id": "6yExhaebs-nD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por favor, informe os dados da API selecionada:\n",
        "\n",
        "**API**: Reddit - PRAW (Python Reddit API Wrapper)\n",
        "\n",
        "**Site oficial**: https://praw.readthedocs.io/\n",
        "\n",
        "**Link para a documentação oficial**: https://praw.readthedocs.io/\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DjJM_qhEZRy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE**: não é necessário usar a mesma **API** da `ATIVIDADE PRÁTICA 03`. Cada **API** pode ser usada por até 4 equipes."
      ],
      "metadata": {
        "id": "bTODq98Myt_u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtjgWQRzNphL"
      },
      "source": [
        "### **DESCRIÇÃO**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementar um `notebook` no `Google Colab` que faça uso do framework **`LangChain`** (obrigatório) e de um **LLM** aplicando, no mínimo, uma técnica de PLN. A técnica pode ser aplicada em qualquer córpus. Também é obrigatório usar uma **API** da `ATIVIDADE PRÁTICA 03`. A **API** pode ser usada tanto para obter os dados quanto para disponibilizar os resultados.\n",
        "\n",
        "O **LLM** e a **API** selecionados devem ser informados na seguinte planilha:\n",
        "\n",
        "> https://docs.google.com/spreadsheets/d/1cOL7zVNffqmliuv23zFm1UJjhEXTdp1Zm5EyAJmhiKg/edit?usp=sharing\n",
        "\n",
        ">\n",
        "As seguintes técnicas de PLN podem ser usadas:\n",
        "\n",
        "*   Correção Gramatical\n",
        "*   Classificação de Textos\n",
        "*   Análise de Sentimentos\n",
        "*   Detecção de Emoções\n",
        "*   Extração de Palavras-chave\n",
        "*   Tradução de Textos\n",
        "*   Sumarização de Textos\n",
        "*   Similaridade de Textos\n",
        "*   Reconhecimento de Entidades Nomeadas\n",
        "*   Sistemas de Perguntas e Respostas\n",
        "\n",
        ">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Lista de APIs:**\n",
        "\n",
        "\n",
        "* YouTube\n",
        "* LinkedIn\n",
        "* Twitter (X)\n",
        "* Facebook\n",
        "* Instagram\n",
        "* Medium\n",
        "* Reddit\n",
        "* TikTok\n",
        "* GitHub\n",
        "* Pinterest\n",
        "* Telegram\n",
        "* Dados financeiros\n",
        "* Notícias\n",
        "* Mercado de Ações\n",
        "* Dados financeiros\n",
        "* SMS\n",
        "* OpenAlex\n",
        "* Whisper (OpenAI)\n",
        "* Discord\n",
        "* Slack\n",
        "* Chuck Norris Jokes\n",
        "* Wikipedia\n",
        "* Last.fm\n",
        "* New York Times\n",
        "* Nasdaq Data Link\n",
        "* Yahoo! Finance\n",
        "* Twilio SendGrid Mail Send\n",
        "* Spotify\n",
        "* Awesome API\n",
        "* Google Books API\n",
        "* Mercado Livre API\n",
        "\n",
        ">\n",
        "\n",
        "**PLANILHA DA ATIVIDADE PRÁTICA 03:**\n",
        "\n",
        "> https://docs.google.com/spreadsheets/d/1-Q1szJ3UmoE2_3LtcRQyqid5fPIcnpsR3XAPnoxLj2o/edit?usp=sharing\n",
        "\n",
        "\n",
        "**IMPORTANTE:** É obrigatório usar o e-mail da UFABC.\n"
      ],
      "metadata": {
        "id": "fXTwkiiGs2BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CRITÉRIOS DE AVALIAÇÃO**\n",
        "---\n"
      ],
      "metadata": {
        "id": "gWsBYQNtxmum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Serão considerados como critérios de avaliação os segunintes pontos:\n",
        "\n",
        "* Uso do framework **`LangChain`**.\n",
        "\n",
        "* Escolha e uso de um **LLM**.\n",
        "\n",
        "* Escolha e uso de uma **API**\n",
        "\n",
        "* Criatividade no uso do framework **`LangChain`** em conjunto com o **LLM** e a **API**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5iHdx4BXYruQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE**: todo o código do notebook deve ser executado. Código sem execução não será considerado."
      ],
      "metadata": {
        "id": "LhwdrMp123Xx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMPLEMENTAÇÃO**\n",
        "---"
      ],
      "metadata": {
        "id": "nw09lujGvfjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# por favor, inserir o código a partir daqui...\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RyUailD5vi9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **DISCLAIMER**"
      ],
      "metadata": {
        "id": "U23GLpu1GY-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eu tive MUITA dificuldade com essa atividade.\n",
        "\n",
        "Testei vários LLMs e não consegui acertar a configuração ou autenticação deles, além de não encontrar um que conseguisse responder em português.\n",
        "\n",
        "Uma coisa que percebi, também, foi que eles são muitas respostas erradas e não consegui entender direito o motivo. MUITAS VEZES, ELE SIMPLESMENTE DELIRA...\n",
        "\n",
        "Sendo sincero, não consegui entender se fui eu o culpado ou se esse modelos são assim mesmo\n",
        "\n",
        "No fim das contas, configurei esse llama.cpp, a muito custo, mas não consegui configurar o ambiente para que a GPu fosse utilizada, o que tornou as respostas muito lentas (elas ficam carregando caractere por caractere).\n",
        "\n",
        "Espero que a estrutura do que propus compensem essas dificuldades, como o uso da API e tentativa de cobrir várias técnicas.\n",
        "\n",
        "Não achei que valia a pena procurar algum tutorial com código pronto e simplesmente colar aqui, sem nem entender o que está acontecendo e por um método de tentativa e erro, então deixei assim mesmo.\n",
        "\n",
        "Outro ponto: eu não sei se llama 2 e llama.cpp são o mesmo LLM, então pode ser que eu tenha repetido o de outros grupos, mas realmente foi o único que consegui rodar, então gostaria de pedir que relevasse isso."
      ],
      "metadata": {
        "id": "eBdbF5gMGf9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Teste com o llama.cpp**"
      ],
      "metadata": {
        "id": "jHsHEC_686ch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instala o llama.cpp para uso com GPU (apesar de eu não ter conseguido fazer com que a GPU do ambiente do Colab fosse utilizada)"
      ],
      "metadata": {
        "id": "J6wNI7bXH1VG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTugptbr9Pp9",
        "outputId": "030bef97-870b-42e8-bf11-290a1dfea7b0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.22.tar.gz (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.22-cp310-cp310-manylinux_2_35_x86_64.whl size=2045692 sha256=def779bc77932ddaae447ad30db0af8a69eb82a2da6131cc135f753adc64b1d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/7e/c4/11fee2bb4b914968fabb2168c237ab1ade9702cfd2c274c4bd\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.2.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instala o LangChain"
      ],
      "metadata": {
        "id": "YggjfnQDI_vA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Lz6No8j9u1O",
        "outputId": "3b73f9a4-488d-44b6-a775-1f3ed36e8c68"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.350-py3-none-any.whl (809 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.2 (from langchain)\n",
            "  Downloading langchain_community-0.0.2-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1 (from langchain)\n",
            "  Downloading langchain_core-0.1.0-py3-none-any.whl (189 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.1/189.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.63 (from langchain)\n",
            "  Downloading langsmith-0.0.69-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.350 langchain-community-0.0.2 langchain-core-0.1.0 langsmith-0.0.69 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_0.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHfYQvtB-EQF",
        "outputId": "a157e37d-0339-4941-d75d-433fb7ad7b68"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-13 02:38:33--  https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_0.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.17, 18.164.174.118, 18.164.174.55, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.17|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: /TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_0.gguf [following]\n",
            "--2023-12-13 02:38:33--  https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_0.gguf\n",
            "Reusing existing connection to huggingface.co:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/b0/ca/b0cae82fd4b3a362cab01d17953c45edac67d1c2dfb9fbb9e69c80c32dc2012e/0d55c4133964f80ee31997853cb83637ae3cc258638b7feae9d1aa5606a895ee?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.Q5_0.gguf%3B+filename%3D%22llama-2-7b-chat.Q5_0.gguf%22%3B&Expires=1702694313&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMjY5NDMxM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iMC9jYS9iMGNhZTgyZmQ0YjNhMzYyY2FiMDFkMTc5NTNjNDVlZGFjNjdkMWMyZGZiOWZiYjllNjljODBjMzJkYzIwMTJlLzBkNTVjNDEzMzk2NGY4MGVlMzE5OTc4NTNjYjgzNjM3YWUzY2MyNTg2MzhiN2ZlYWU5ZDFhYTU2MDZhODk1ZWU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=vE45WWQuFeXGfPohNFkVI-KoUNPgI22Ft6RdBaZOheVTuVG6Sg30SfX2Ovni4LU7jGO0wRqajRI%7E5cR%7EWUmu0cD7hL07enkDrddT2Zab0AwskTAn02-Wg87OJTfRawVcIveccz75clMnXTi2e01xPpPVYsl%7EYbRe4N7Afc93%7E9qiG9laqsaZ1T-M6K0sScMcIVaEA%7ENyGJIxkMrvr5ZLZYOOW2RCTZpR4IY7CqBBnblv8-6BQw5J1GFFe-cqeWP8kfI1-rrsVfccN8%7EWP4YEPKOAe9LxtJl3WSuDNUVbN%7EyIeRCJLqXctnt0weZzOrzl8gnY%7EdXMrPa-8aBDlCDk8A__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-12-13 02:38:33--  https://cdn-lfs.huggingface.co/repos/b0/ca/b0cae82fd4b3a362cab01d17953c45edac67d1c2dfb9fbb9e69c80c32dc2012e/0d55c4133964f80ee31997853cb83637ae3cc258638b7feae9d1aa5606a895ee?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.Q5_0.gguf%3B+filename%3D%22llama-2-7b-chat.Q5_0.gguf%22%3B&Expires=1702694313&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMjY5NDMxM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iMC9jYS9iMGNhZTgyZmQ0YjNhMzYyY2FiMDFkMTc5NTNjNDVlZGFjNjdkMWMyZGZiOWZiYjllNjljODBjMzJkYzIwMTJlLzBkNTVjNDEzMzk2NGY4MGVlMzE5OTc4NTNjYjgzNjM3YWUzY2MyNTg2MzhiN2ZlYWU5ZDFhYTU2MDZhODk1ZWU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=vE45WWQuFeXGfPohNFkVI-KoUNPgI22Ft6RdBaZOheVTuVG6Sg30SfX2Ovni4LU7jGO0wRqajRI%7E5cR%7EWUmu0cD7hL07enkDrddT2Zab0AwskTAn02-Wg87OJTfRawVcIveccz75clMnXTi2e01xPpPVYsl%7EYbRe4N7Afc93%7E9qiG9laqsaZ1T-M6K0sScMcIVaEA%7ENyGJIxkMrvr5ZLZYOOW2RCTZpR4IY7CqBBnblv8-6BQw5J1GFFe-cqeWP8kfI1-rrsVfccN8%7EWP4YEPKOAe9LxtJl3WSuDNUVbN%7EyIeRCJLqXctnt0weZzOrzl8gnY%7EdXMrPa-8aBDlCDk8A__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.154.206.17, 18.154.206.4, 18.154.206.28, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.154.206.17|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4651691712 (4.3G) [binary/octet-stream]\n",
            "Saving to: ‘llama-2-7b-chat.Q5_0.gguf’\n",
            "\n",
            "llama-2-7b-chat.Q5_ 100%[===================>]   4.33G   137MB/s    in 38s     \n",
            "\n",
            "2023-12-13 02:39:11 (118 MB/s) - ‘llama-2-7b-chat.Q5_0.gguf’ saved [4651691712/4651691712]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importa as bibliotecas do LangChain"
      ],
      "metadata": {
        "id": "y2I9e8kVKNQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
      ],
      "metadata": {
        "id": "Lc6V5U8t-Gly"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cria o template para o prompt"
      ],
      "metadata": {
        "id": "fQrzFyefKT3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "USj5WIpg-Nmq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
      ],
      "metadata": {
        "id": "snd7ELtS-Ov-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configura o LLM (nesse caso, está usando o LLaMA 2 7B)"
      ],
      "metadata": {
        "id": "QYT7R7VNKqAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
        "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"/content/llama-2-7b-chat.Q5_0.gguf\",\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,  # Verbose is required to pass to the callback manager\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2_c93Zi-UXL",
        "outputId": "e151ae06-f704-42dc-fa9b-e629a023d4c1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pergunta teste (não consegui fazer com que ele respondesse em português ou traduzisse sem delirar...)"
      ],
      "metadata": {
        "id": "C3rr11VTKzpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "question = \"Who were the last five presidents of Brazil??\"\n",
        "llm_chain.run(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "lY5VoAzqBInT",
        "outputId": "b548728d-3d77-4ee6-8c00-c3eb87ddeaea"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Last 5 presidentes do Brasil (Last 5 presidents of Brazil) - Who were they?\n",
            "2. The last five presidents of Brazil are:\n",
            "a) Luiz Inácio Lula da Silva (2003-2011)\n",
            "b) Dilma Rousseff (2011-2016)\n",
            "c) Michel Temer (2016-2019)\n",
            "d) Jair Bolsonaro (2019-presente)\n",
            "e) Fernando Haddad (2019)"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n1. Last 5 presidentes do Brasil (Last 5 presidents of Brazil) - Who were they?\\n2. The last five presidents of Brazil are:\\na) Luiz Inácio Lula da Silva (2003-2011)\\nb) Dilma Rousseff (2011-2016)\\nc) Michel Temer (2016-2019)\\nd) Jair Bolsonaro (2019-presente)\\ne) Fernando Haddad (2019)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note que a resposta, pelo menos na minha opinião, contém muitos erros factuais e de formatação, o que, como comentei lá no disclaimer, não sei se foi falha minha ou se o modelo não é bom. Haddad não foi presidente e o modelo mistura os dois idiomas na mesma frase, por exemplo."
      ],
      "metadata": {
        "id": "lc-1Zx32K8iW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r5YVZbLQBPGj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}