{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gustex/PLN_2023_Atividade_05/blob/main/PLN_Atividade_Pr%C3%A1tica_05_Gustavo_Teixeira.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2023.Q3]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m67OOx9MX_3"
      },
      "source": [
        "### **ATIVIDADE PRÁTICA 05 [LangChain + Grandes Modelos de Linguagem + API]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gk0nHKabBT-"
      },
      "source": [
        "A **ATIVIDADE PRÁTICA 05** deve ser feita utilizando o **Google Colab** com uma conta sua vinculada ao Gmail. O link do seu notebook, armazenado no Google Drive, além do link de um repositório no GitHub e os principais resultados da atividade, devem ser enviados usando o seguinte formulário:\n",
        "\n",
        "> https://forms.gle/C1oUi1FKTZ4W9fNdA\n",
        "\n",
        "\n",
        "**IMPORTANTE**: A submissão deve ser feita até o dia **12/12 (terça-feira)** APENAS POR UM INTEGRANTE DA EQUIPE, até às 23h59. Por favor, lembre-se de dar permissão de ACESSO IRRESTRITO para o professor da disciplina de PLN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7hJlilKM485"
      },
      "source": [
        "### **EQUIPE**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POR FAVOR, PREENCHER OS INTEGRANDES DA SUA EQUIPE:**\n",
        "\n",
        "\n",
        "**Integrante 01:**\n",
        "\n",
        "`Por favor, informe o seu nome completo e RA:`\n",
        "\n",
        "Gustavo da Silva Teixeira - RA 11201812255"
      ],
      "metadata": {
        "id": "tnIArN0QY-Ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GRANDE MODELO DE LINGUAGEM (*Large Language Model - LLM*)**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "VbYD2mw8y4CN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cada equipe deve selecionar um Grande Modelo de Linguagem (*Large Language Model - LMM*). Preferencialmente, usar um modelo gratuito. Cada modelo pode ser escolhido por até 4 equipes.\n",
        "\n",
        ">\n",
        "\n",
        "Uma lista de LLMs está disponível em:\n",
        "\n",
        "> https://integrations.langchain.com/llms\n",
        "> https://python.langchain.com/docs/integrations/llms/"
      ],
      "metadata": {
        "id": "_UlblxFxzDV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelos da Hugging Face:**\n",
        "\n",
        "* Mistral Base: modelo promissor.\n",
        "\n",
        "> https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\n",
        "\n",
        "* Mistral Quantizado: mais leve. Pode ser um pouco mais difícil de configurar, mas deve ocupar menos memória.\n",
        "\n",
        "> https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF\n",
        "\n",
        "* Mistral Lite da Amazon: pode ter desempenho melhor.\n",
        "\n",
        "> https://huggingface.co/amazon/MistralLite\n",
        "\n",
        "* Llama 2 7B: modelo da Meta melhorado.\n",
        "\n",
        "> https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
        "\n",
        "* Llama 2 7b 32k: contexto maior pelo mesmo tamanho.\n",
        "\n",
        "> https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct\n",
        "\n",
        "* Galactica: para artigos científicos\n",
        "\n",
        "> https://huggingface.co/facebook/galactica-6.7b\n",
        "\n",
        "* Alpaca: alternativo ao Llama\n",
        "\n",
        "> https://huggingface.co/chavinlo/alpaca-native\n",
        "\n",
        "> https://huggingface.co/spaces/tloen/alpaca-lora"
      ],
      "metadata": {
        "id": "GVpiVhzn3QqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lista de Modelos Interessantes:**\n",
        "\n",
        "> https://www.kdnuggets.com/2023/04/8-opensource-alternative-chatgpt-bard.html\n",
        "\n",
        "* Vicuna:\n",
        "> https://huggingface.co/lmsys/vicuna-7b-v1.5-16k\n",
        "\n",
        "* Openchat kit:\n",
        "> https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B\n",
        "\n",
        "* Meta OPT:\n",
        "> https://huggingface.co/facebook/opt-1.3b\n"
      ],
      "metadata": {
        "id": "WRuAiSr05Ayt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**APIs:**\n",
        "\n",
        "* GPT4ALL: tenta ser um chatGPT aberto\n",
        "> https://python.langchain.com/docs/integrations/llms/gpt4all\n",
        "\n",
        "* YandexGPT: da empresa russa que criou um modelo de *Machine Learning* clássico muito bom, o CatBoost.\n",
        "> https://python.langchain.com/docs/integrations/llms/yandex\n",
        "\n",
        "* Anthropic: empresa forte concorrente da OpenAi.\n",
        "> https://python.langchain.com/docs/integrations/chat/anthropic\n",
        "\n",
        "* Gorilla: pipeline para geração de código\n",
        " > https://www.kdnuggets.com/2023/06/meet-gorilla-uc-berkeley-microsoft-apiaugmented-llm-outperforms-gpt4-chatgpt-claude.html"
      ],
      "metadata": {
        "id": "5Q3mhqda5WP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE**: todo esse levantamento e comentários foram feitos pelo aluno  **Bruno Sanches Rodrigues**."
      ],
      "metadata": {
        "id": "DO6XMjHx58u9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por favor, informe os dados do LLM selecionada:\n",
        "\n",
        ">\n",
        "\n",
        "\n",
        "**LLM**:\n",
        "\n",
        ">LLaMA.cpp (LLaMA 7 2B)\n",
        "\n",
        "**Link para a documentação oficial**:\n",
        "\n",
        ">https://python.langchain.com/docs/integrations/llms/llamacpp\n",
        "\n",
        "\n",
        "**Site oficial (GitHub)**:\n",
        "\n",
        ">https://github.com/abetlen/llama-cpp-python"
      ],
      "metadata": {
        "id": "a6AkE6iW0c3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE**: não pode ser o modelo da `OpenAI`."
      ],
      "metadata": {
        "id": "A2oo8GtK0xSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **API**\n",
        "---"
      ],
      "metadata": {
        "id": "6yExhaebs-nD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por favor, informe os dados da API selecionada:\n",
        "\n",
        "**API**: Reddit - PRAW (Python Reddit API Wrapper)\n",
        "\n",
        "**Site oficial**: https://praw.readthedocs.io/\n",
        "\n",
        "**Link para a documentação oficial**: https://praw.readthedocs.io/\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DjJM_qhEZRy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE**: não é necessário usar a mesma **API** da `ATIVIDADE PRÁTICA 03`. Cada **API** pode ser usada por até 4 equipes."
      ],
      "metadata": {
        "id": "bTODq98Myt_u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtjgWQRzNphL"
      },
      "source": [
        "### **DESCRIÇÃO**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementar um `notebook` no `Google Colab` que faça uso do framework **`LangChain`** (obrigatório) e de um **LLM** aplicando, no mínimo, uma técnica de PLN. A técnica pode ser aplicada em qualquer córpus. Também é obrigatório usar uma **API** da `ATIVIDADE PRÁTICA 03`. A **API** pode ser usada tanto para obter os dados quanto para disponibilizar os resultados.\n",
        "\n",
        "O **LLM** e a **API** selecionados devem ser informados na seguinte planilha:\n",
        "\n",
        "> https://docs.google.com/spreadsheets/d/1cOL7zVNffqmliuv23zFm1UJjhEXTdp1Zm5EyAJmhiKg/edit?usp=sharing\n",
        "\n",
        ">\n",
        "As seguintes técnicas de PLN podem ser usadas:\n",
        "\n",
        "*   Correção Gramatical\n",
        "*   Classificação de Textos\n",
        "*   Análise de Sentimentos\n",
        "*   Detecção de Emoções\n",
        "*   Extração de Palavras-chave\n",
        "*   Tradução de Textos\n",
        "*   Sumarização de Textos\n",
        "*   Similaridade de Textos\n",
        "*   Reconhecimento de Entidades Nomeadas\n",
        "*   Sistemas de Perguntas e Respostas\n",
        "\n",
        ">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Lista de APIs:**\n",
        "\n",
        "\n",
        "* YouTube\n",
        "* LinkedIn\n",
        "* Twitter (X)\n",
        "* Facebook\n",
        "* Instagram\n",
        "* Medium\n",
        "* Reddit\n",
        "* TikTok\n",
        "* GitHub\n",
        "* Pinterest\n",
        "* Telegram\n",
        "* Dados financeiros\n",
        "* Notícias\n",
        "* Mercado de Ações\n",
        "* Dados financeiros\n",
        "* SMS\n",
        "* OpenAlex\n",
        "* Whisper (OpenAI)\n",
        "* Discord\n",
        "* Slack\n",
        "* Chuck Norris Jokes\n",
        "* Wikipedia\n",
        "* Last.fm\n",
        "* New York Times\n",
        "* Nasdaq Data Link\n",
        "* Yahoo! Finance\n",
        "* Twilio SendGrid Mail Send\n",
        "* Spotify\n",
        "* Awesome API\n",
        "* Google Books API\n",
        "* Mercado Livre API\n",
        "\n",
        ">\n",
        "\n",
        "**PLANILHA DA ATIVIDADE PRÁTICA 03:**\n",
        "\n",
        "> https://docs.google.com/spreadsheets/d/1-Q1szJ3UmoE2_3LtcRQyqid5fPIcnpsR3XAPnoxLj2o/edit?usp=sharing\n",
        "\n",
        "\n",
        "**IMPORTANTE:** É obrigatório usar o e-mail da UFABC.\n"
      ],
      "metadata": {
        "id": "fXTwkiiGs2BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CRITÉRIOS DE AVALIAÇÃO**\n",
        "---\n"
      ],
      "metadata": {
        "id": "gWsBYQNtxmum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Serão considerados como critérios de avaliação os segunintes pontos:\n",
        "\n",
        "* Uso do framework **`LangChain`**.\n",
        "\n",
        "* Escolha e uso de um **LLM**.\n",
        "\n",
        "* Escolha e uso de uma **API**\n",
        "\n",
        "* Criatividade no uso do framework **`LangChain`** em conjunto com o **LLM** e a **API**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5iHdx4BXYruQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE**: todo o código do notebook deve ser executado. Código sem execução não será considerado."
      ],
      "metadata": {
        "id": "LhwdrMp123Xx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMPLEMENTAÇÃO**\n",
        "---"
      ],
      "metadata": {
        "id": "nw09lujGvfjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# por favor, inserir o código a partir daqui...\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RyUailD5vi9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **AVISO IMPORTANTE (\"DISCLAIMER\")**"
      ],
      "metadata": {
        "id": "U23GLpu1GY-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eu tive MUITA dificuldade com essa atividade.\n",
        "\n",
        "Testei vários LLMs e não consegui acertar a configuração ou autenticação deles, além de não encontrar um que conseguisse responder em português.\n",
        "\n",
        "Uma coisa que percebi, também, foi que eles dão muitas respostas erradas e não consegui entender direito o motivo. MUITAS VEZES, ELE SIMPLESMENTE DELIRA...\n",
        "\n",
        "Sendo sincero, não consegui entender se fui eu o culpado ou se esse modelos são assim mesmo.\n",
        "\n",
        "No fim das contas, configurei esse LLaMA.cpp, a muito custo, mas não consegui configurar o ambiente para que a GPU fosse utilizada, o que tornou as respostas muito lentas (elas ficam carregando caractere por caractere).\n",
        "\n",
        "Espero que a estrutura do que propus compense essas dificuldades que tive, como o uso da API e tentativa de cobrir várias técnicas.\n",
        "\n",
        "Não achei que valia a pena procurar algum tutorial com código pronto e simplesmente colar aqui, sem nem entender o que está acontecendo e por um método de tentativa e erro, então deixei assim mesmo.\n",
        "\n",
        "Outro ponto: eu não sei se LLaMA 2 e LLaMA.cpp são o mesmo LLM, então pode ser que eu tenha repetido o de outros grupos, mas realmente foi o único que consegui rodar, então gostaria de pedir que relevasse isso.\n",
        "\n",
        "O LLM em que mais consegui avançar além do LLaMA foi o Flan-T5 da Google, usando o Hugging Face Hub, mas em poucos exemplos o achei muito delirante, também.\n",
        "\n",
        "**ATENÇÃO: As células marcadas como erro ou foram deixadas propositalmente ou são execuções interrompidas maualmente por mim. Em ambos os casos, deixo uma explicação na célula seguinte.**"
      ],
      "metadata": {
        "id": "eBdbF5gMGf9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Teste com o LLaMA.cpp**"
      ],
      "metadata": {
        "id": "jHsHEC_686ch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instala o LLaMA.cpp para uso com GPU (apesar de eu não ter conseguido fazer com que a GPU do ambiente do Colab fosse utilizada)"
      ],
      "metadata": {
        "id": "J6wNI7bXH1VG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTugptbr9Pp9",
        "outputId": "030bef97-870b-42e8-bf11-290a1dfea7b0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.22.tar.gz (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.22-cp310-cp310-manylinux_2_35_x86_64.whl size=2045692 sha256=def779bc77932ddaae447ad30db0af8a69eb82a2da6131cc135f753adc64b1d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/7e/c4/11fee2bb4b914968fabb2168c237ab1ade9702cfd2c274c4bd\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.2.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instala o LangChain"
      ],
      "metadata": {
        "id": "YggjfnQDI_vA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Lz6No8j9u1O",
        "outputId": "3b73f9a4-488d-44b6-a775-1f3ed36e8c68"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.350-py3-none-any.whl (809 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.2 (from langchain)\n",
            "  Downloading langchain_community-0.0.2-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1 (from langchain)\n",
            "  Downloading langchain_core-0.1.0-py3-none-any.whl (189 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.1/189.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.63 (from langchain)\n",
            "  Downloading langsmith-0.0.69-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.350 langchain-community-0.0.2 langchain-core-0.1.0 langsmith-0.0.69 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_0.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHfYQvtB-EQF",
        "outputId": "a157e37d-0339-4941-d75d-433fb7ad7b68"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-13 02:38:33--  https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_0.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.17, 18.164.174.118, 18.164.174.55, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.17|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: /TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_0.gguf [following]\n",
            "--2023-12-13 02:38:33--  https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_0.gguf\n",
            "Reusing existing connection to huggingface.co:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/b0/ca/b0cae82fd4b3a362cab01d17953c45edac67d1c2dfb9fbb9e69c80c32dc2012e/0d55c4133964f80ee31997853cb83637ae3cc258638b7feae9d1aa5606a895ee?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.Q5_0.gguf%3B+filename%3D%22llama-2-7b-chat.Q5_0.gguf%22%3B&Expires=1702694313&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMjY5NDMxM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iMC9jYS9iMGNhZTgyZmQ0YjNhMzYyY2FiMDFkMTc5NTNjNDVlZGFjNjdkMWMyZGZiOWZiYjllNjljODBjMzJkYzIwMTJlLzBkNTVjNDEzMzk2NGY4MGVlMzE5OTc4NTNjYjgzNjM3YWUzY2MyNTg2MzhiN2ZlYWU5ZDFhYTU2MDZhODk1ZWU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=vE45WWQuFeXGfPohNFkVI-KoUNPgI22Ft6RdBaZOheVTuVG6Sg30SfX2Ovni4LU7jGO0wRqajRI%7E5cR%7EWUmu0cD7hL07enkDrddT2Zab0AwskTAn02-Wg87OJTfRawVcIveccz75clMnXTi2e01xPpPVYsl%7EYbRe4N7Afc93%7E9qiG9laqsaZ1T-M6K0sScMcIVaEA%7ENyGJIxkMrvr5ZLZYOOW2RCTZpR4IY7CqBBnblv8-6BQw5J1GFFe-cqeWP8kfI1-rrsVfccN8%7EWP4YEPKOAe9LxtJl3WSuDNUVbN%7EyIeRCJLqXctnt0weZzOrzl8gnY%7EdXMrPa-8aBDlCDk8A__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-12-13 02:38:33--  https://cdn-lfs.huggingface.co/repos/b0/ca/b0cae82fd4b3a362cab01d17953c45edac67d1c2dfb9fbb9e69c80c32dc2012e/0d55c4133964f80ee31997853cb83637ae3cc258638b7feae9d1aa5606a895ee?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.Q5_0.gguf%3B+filename%3D%22llama-2-7b-chat.Q5_0.gguf%22%3B&Expires=1702694313&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMjY5NDMxM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iMC9jYS9iMGNhZTgyZmQ0YjNhMzYyY2FiMDFkMTc5NTNjNDVlZGFjNjdkMWMyZGZiOWZiYjllNjljODBjMzJkYzIwMTJlLzBkNTVjNDEzMzk2NGY4MGVlMzE5OTc4NTNjYjgzNjM3YWUzY2MyNTg2MzhiN2ZlYWU5ZDFhYTU2MDZhODk1ZWU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=vE45WWQuFeXGfPohNFkVI-KoUNPgI22Ft6RdBaZOheVTuVG6Sg30SfX2Ovni4LU7jGO0wRqajRI%7E5cR%7EWUmu0cD7hL07enkDrddT2Zab0AwskTAn02-Wg87OJTfRawVcIveccz75clMnXTi2e01xPpPVYsl%7EYbRe4N7Afc93%7E9qiG9laqsaZ1T-M6K0sScMcIVaEA%7ENyGJIxkMrvr5ZLZYOOW2RCTZpR4IY7CqBBnblv8-6BQw5J1GFFe-cqeWP8kfI1-rrsVfccN8%7EWP4YEPKOAe9LxtJl3WSuDNUVbN%7EyIeRCJLqXctnt0weZzOrzl8gnY%7EdXMrPa-8aBDlCDk8A__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.154.206.17, 18.154.206.4, 18.154.206.28, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.154.206.17|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4651691712 (4.3G) [binary/octet-stream]\n",
            "Saving to: ‘llama-2-7b-chat.Q5_0.gguf’\n",
            "\n",
            "llama-2-7b-chat.Q5_ 100%[===================>]   4.33G   137MB/s    in 38s     \n",
            "\n",
            "2023-12-13 02:39:11 (118 MB/s) - ‘llama-2-7b-chat.Q5_0.gguf’ saved [4651691712/4651691712]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importa as bibliotecas do LangChain"
      ],
      "metadata": {
        "id": "y2I9e8kVKNQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
      ],
      "metadata": {
        "id": "Lc6V5U8t-Gly"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cria o template para o prompt"
      ],
      "metadata": {
        "id": "fQrzFyefKT3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "USj5WIpg-Nmq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
      ],
      "metadata": {
        "id": "snd7ELtS-Ov-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configura o LLM (nesse caso, está usando o LLaMA 2 7B)"
      ],
      "metadata": {
        "id": "QYT7R7VNKqAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
        "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"/content/llama-2-7b-chat.Q5_0.gguf\",\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,  # Verbose is required to pass to the callback manager\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2_c93Zi-UXL",
        "outputId": "e151ae06-f704-42dc-fa9b-e629a023d4c1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pergunta teste (não consegui fazer com que ele respondesse em português ou traduzisse sem delirar...)"
      ],
      "metadata": {
        "id": "C3rr11VTKzpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "question = \"Who were the last five presidents of Brazil??\"\n",
        "llm_chain.run(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "lY5VoAzqBInT",
        "outputId": "b548728d-3d77-4ee6-8c00-c3eb87ddeaea"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Last 5 presidentes do Brasil (Last 5 presidents of Brazil) - Who were they?\n",
            "2. The last five presidents of Brazil are:\n",
            "a) Luiz Inácio Lula da Silva (2003-2011)\n",
            "b) Dilma Rousseff (2011-2016)\n",
            "c) Michel Temer (2016-2019)\n",
            "d) Jair Bolsonaro (2019-presente)\n",
            "e) Fernando Haddad (2019)"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n1. Last 5 presidentes do Brasil (Last 5 presidents of Brazil) - Who were they?\\n2. The last five presidents of Brazil are:\\na) Luiz Inácio Lula da Silva (2003-2011)\\nb) Dilma Rousseff (2011-2016)\\nc) Michel Temer (2016-2019)\\nd) Jair Bolsonaro (2019-presente)\\ne) Fernando Haddad (2019)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note que a resposta, pelo menos na minha opinião, contém muitos erros factuais e de formatação, o que, como comentei lá no disclaimer, não sei se foi falha minha ou se o modelo não é bom. Haddad não foi presidente e o modelo mistura os dois idiomas na mesma frase, por exemplo."
      ],
      "metadata": {
        "id": "lc-1Zx32K8iW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configurando o PRAW, a API do Reddit"
      ],
      "metadata": {
        "id": "mU3UEqSyOVl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui, os passos são os que usei na minha Atividade 3, então não detalharei como é feita a autenticação, para não tomar muito espaço."
      ],
      "metadata": {
        "id": "Xva1vFB2Odq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TC877aZOpXW",
        "outputId": "920d94ae-dbfc-4817-aabd-f540b9d48236"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.7.1-py3-none-any.whl (191 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prawcore<3,>=2.1 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Collecting update-checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.7.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2023.11.17)\n",
            "Installing collected packages: update-checker, prawcore, praw\n",
            "Successfully installed praw-7.7.1 prawcore-2.4.0 update-checker-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "    client_id     = \"OmXSJ1TPNOOXjJD17rpEBA\",\n",
        "    client_secret = \"ud7X2KPVRfVp88TZQRMcx1cknspC7A\",\n",
        "    user_agent    = \"Tutorial PRAW do Gustavo\",\n",
        "    username      = \"Teste_Reddit_Python\",\n",
        "    password      = \"TestePraw147258*\",\n",
        ")"
      ],
      "metadata": {
        "id": "eh0muDftPB6w"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reddit.user.me())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VnGUeNMQegJ",
        "outputId": "d85d9aab-b39c-43a5-bd25-33c4b41f3b69"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teste_Reddit_Python\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O subreddit que vou usar para essa atividade é o r/AskReddit, onde os usuários podem faz pergutnas sobre quaisquer assuntos.\n",
        "\n",
        "**Pelo fato do LLM que consegui configurar só responder em inglês, escolhi manter todo o texto da atividade em inglês também, por isso essa escolha de subreddit. Além disso, um fórum de perguntas variadas pode ser interessante para uma atividade onde queremos trabalhar apenas com texto e de preferência com vocabulário amplo.**\n",
        "\n",
        "Note que o que farei aqui usa as mesmas ferramentas que descrevi na Atividade 3, apenas alterando os textos para inglês"
      ],
      "metadata": {
        "id": "qyRLGNrWQ_jB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Acessar um subreddit específico\n",
        "subreddit = reddit.subreddit(\"AskReddit\")\n",
        "\n",
        "# Exibir informações sobre o subreddit\n",
        "print(f\"Name: {subreddit.display_name}\")\n",
        "print(f\"Subscribers: {subreddit.subscribers}\")\n",
        "print(f\"Description: {subreddit.description}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvjY76bwQe4M",
        "outputId": "51a3457a-f294-4a52-d286-f821edf20273"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: AskReddit\n",
            "Subscribers: 44221309\n",
            "Description: ###### [ [ SERIOUS ] ](http://www.reddit.com/r/askreddit/submit?selftext=true&title=%5BSerious%5D)\r\n",
            "\r\n",
            "\r\n",
            "##### [Rules](https://www.reddit.com/r/AskReddit/wiki/index#wiki_rules):\r\n",
            "1. You must post a clear and direct question in the title. The title may contain two, short, necessary context sentences.\r\n",
            "No text is allowed in the textbox. Your thoughts/responses to the question can go in the comments section. [more >>](https://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_1-)\r\n",
            "\r\n",
            "2. Any post asking for advice should be generic and not specific to your situation alone. [more >>](https://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_2-)\r\n",
            "\r\n",
            "3. AskReddit is for open-ended discussion questions. [more >>](https://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_3-)\r\n",
            "\r\n",
            "4. Posting, or seeking, any identifying personal information, real or fake, will result in a ban without a prior warning. [more >>](https://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_4-)\r\n",
            "\r\n",
            "5. AskReddit is not your soapbox, personal army, or advertising platform. [more >>](https://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_5-)\r\n",
            "\r\n",
            "6. [Serious] tagged posts are off-limits to jokes or irrelevant replies. [more >>](https://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_6-)\r\n",
            "\r\n",
            "7. Soliciting money, goods, services, or favours is not allowed. [more >>](https://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_7-)\r\n",
            "\r\n",
            "8. Mods reserve the right to remove content or restrict users' posting privileges as necessary if it is deemed detrimental to the subreddit or to the experience of others. [more >>](https://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_8-)\r\n",
            "\r\n",
            "9. Comment replies consisting solely of images will be removed. [more >>](https://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_9-)\r\n",
            "\r\n",
            "10. Do not post harmful misinformation. [more >>](https://www.reddit.com/r/AskReddit/wiki/index/#wiki_-rule_10-)\r\n",
            "\r\n",
            "11. Spam, machine-generated content, and karma farming are not permitted. [more >>](https://www.reddit.com/r/AskReddit/wiki/index/#wiki_-rule_11-)\r\n",
            "\r\n",
            "##### If you think your post has disappeared, see spam or an inappropriate post, please do not hesitate to [contact the mods](https://www.reddit.com/message/compose?to=%2Fr%2FAskReddit), we're happy to help.\r\n",
            "\r\n",
            "---\r\n",
            "\r\n",
            "#### Tags to use:\r\n",
            "\r\n",
            "> ## [[Serious]](https://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_6-)\r\n",
            "\r\n",
            "### Use a **[Serious]** post tag to designate your post as a serious, on-topic-only thread.\r\n",
            "\r\n",
            "-\r\n",
            "\r\n",
            "#### Filter posts by subject:\r\n",
            "\r\n",
            "[Mod posts](http://ud.reddit.com/r/AskReddit/#ud)\r\n",
            "[Serious posts](https://www.reddit.com/r/AskReddit/search/?q=flair%3Aserious&sort=new&restrict_sr=on&t=all)\r\n",
            "[Megathread](http://bu.reddit.com/r/AskReddit/#bu)\r\n",
            "[Breaking news](http://nr.reddit.com/r/AskReddit/#nr)\r\n",
            "[Unfilter](/r/AskReddit)\r\n",
            "\r\n",
            "\r\n",
            "-\r\n",
            "\r\n",
            "### Please use spoiler tags to hide spoilers. `>!insert spoiler here!<`\r\n",
            "\r\n",
            "-\r\n",
            "\r\n",
            "#### Other subreddits you might like:\r\n",
            "some|header\r\n",
            ":---|:---\r\n",
            "[Ask Others](https://www.reddit.com/r/AskReddit/wiki/sidebarsubs#wiki_ask_others)|[Self & Others](https://www.reddit.com/r/AskReddit/wiki/sidebarsubs#wiki_self_.26amp.3B_others)\r\n",
            "[Find a subreddit](https://www.reddit.com/r/AskReddit/wiki/sidebarsubs#wiki_find_a_subreddit)|[Learn something](https://www.reddit.com/r/AskReddit/wiki/sidebarsubs#wiki_learn_something)\r\n",
            "[Meta Subs](https://www.reddit.com/r/AskReddit/wiki/sidebarsubs#wiki_meta)|[What is this ___](https://www.reddit.com/r/AskReddit/wiki/sidebarsubs#wiki_what_is_this______)\r\n",
            "[AskReddit Offshoots](https://www.reddit.com/r/AskReddit/wiki/sidebarsubs#wiki_askreddit_offshoots)|[Offers & Assistance](https://www.reddit.com/r/AskReddit/wiki/sidebarsubs#wiki_offers_.26amp.3B_assistance)\r\n",
            "\r\n",
            "\r\n",
            "-\r\n",
            "\r\n",
            "### Ever read the reddiquette? [Take a peek!](/wiki/reddiquette)\r\n",
            "\r\n",
            "[](#/RES_SR_Config/NightModeCompatible)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abaixo, listamos os 20 posts com maior pontuação no subreddit AskReddit e usaremos essa lista para a primeira técnica de PLN."
      ],
      "metadata": {
        "id": "KM-Bn_yoVk11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "top_posts = subreddit.top(limit=20)\n",
        "print(top_posts, \"\\n\")\n",
        "\n",
        "cont = 1\n",
        "\n",
        "#for post in top_posts:\n",
        "#    print('--- Post ', cont, ' ---')\n",
        "#    print('Title: ', post.title)\n",
        "#    print('Text: ', post.selftext)\n",
        "#    print('Subreddit: ', post.subreddit)\n",
        "#    print('ID: ', post.id)\n",
        "#    print('Author: ', post.author)\n",
        "#    print('URL: ', post.url)\n",
        "#    print('Score: ', post.score)\n",
        "#    print('Comments: ', post.num_comments)\n",
        "#    print('Created: ', datetime.fromtimestamp(post.created_utc))\n",
        "#    print()\n",
        "#    cont += 1\n",
        "\n",
        "texto = ''\n",
        "for post in top_posts:\n",
        "    texto += '--- Post ' + str(cont) + ' ---' + '\\n'\n",
        "    texto += 'Title: ' + post.title + '\\n'\n",
        "    texto += 'Text: ' + post.selftext + '\\n'\n",
        "    texto += 'Subreddit: ' + post.subreddit.display_name + '\\n'\n",
        "    texto += 'ID: ' + post.id + '\\n'\n",
        "    if post.author is None:\n",
        "        texto += 'None\\n'\n",
        "    else:\n",
        "      texto += 'Author: ' + post.author.name + '\\n'\n",
        "    texto += 'URL: ' + post.url + '\\n'\n",
        "    texto += 'Score: ' + str(post.score.real) + '\\n'\n",
        "    texto += 'Comments: ' + str(post.num_comments) + '\\n'\n",
        "    texto += 'Created: ' + datetime.fromtimestamp(post.created_utc).strftime(\"%m/%d/%Y, %H:%M:%S\") + '\\n\\n'\n",
        "    cont += 1\n",
        "\n",
        "print(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dol6osLpQ8r_",
        "outputId": "f4732098-962e-4963-dc4a-e047106d1113"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<praw.models.listing.generator.ListingGenerator object at 0x789db4ff3790> \n",
            "\n",
            "--- Post 1 ---\n",
            "Title: People who haven't pooped in 2019 yet, why are you still holding on to last years shit?\n",
            "Text: \n",
            "Subreddit: AskReddit\n",
            "ID: ablzuq\n",
            "Author: ShoddySubstance\n",
            "URL: https://www.reddit.com/r/AskReddit/comments/ablzuq/people_who_havent_pooped_in_2019_yet_why_are_you/\n",
            "Score: 221991\n",
            "Comments: 7949\n",
            "Created: 01/01/2019, 21:06:27\n",
            "\n",
            "--- Post 2 ---\n",
            "Title: How would you feel about Reddit adding 3 NSFW filters to distinguish between porn, gore, and repetitive posts asking how you would feel about Reddit adding 2 NSFW filters to distinguish between porn and gore?\n",
            "Text: \n",
            "Subreddit: AskReddit\n",
            "ID: l7530r\n",
            "Author: SoupIsAHotSmoothie\n",
            "URL: https://www.reddit.com/r/AskReddit/comments/l7530r/how_would_you_feel_about_reddit_adding_3_nsfw/\n",
            "Score: 217922\n",
            "Comments: 2908\n",
            "Created: 01/28/2021, 18:51:22\n",
            "\n",
            "--- Post 3 ---\n",
            "Title: Would you watch a show where a billionaire CEO has to go an entire month on their lowest paid employees salary, without access to any other resources than that of the employee? What do you think would happen?\n",
            "Text: \n",
            "Subreddit: AskReddit\n",
            "ID: f08dxb\n",
            "Author: txhorns1330\n",
            "URL: https://www.reddit.com/r/AskReddit/comments/f08dxb/would_you_watch_a_show_where_a_billionaire_ceo/\n",
            "Score: 197600\n",
            "Comments: 13357\n",
            "Created: 02/07/2020, 09:53:32\n",
            "\n",
            "--- Post 4 ---\n",
            "Title: What if God came down one day and said \"It's pronounced 'Jod' then left?\n",
            "Text: \n",
            "Subreddit: AskReddit\n",
            "ID: iwedc5\n",
            "Author: esi_disi\n",
            "URL: https://www.reddit.com/r/AskReddit/comments/iwedc5/what_if_god_came_down_one_day_and_said_its/\n",
            "Score: 195920\n",
            "Comments: 10271\n",
            "Created: 09/20/2020, 14:01:51\n",
            "\n",
            "--- Post 5 ---\n",
            "Title: How would you feel about a feature where if someone upvotes a crosspost, the original post is upvoted automatically?\n",
            "Text: \n",
            "Subreddit: AskReddit\n",
            "ID: draola\n",
            "Author: Ka1-\n",
            "URL: https://www.reddit.com/r/AskReddit/comments/draola/how_would_you_feel_about_a_feature_where_if/\n",
            "Score: 186430\n",
            "Comments: 2787\n",
            "Created: 11/04/2019, 01:57:46\n",
            "\n",
            "--- Post 6 ---\n",
            "Title: How would you feel about a \"if you accidentally scroll to the top, you can go back to where you were,\" button for Reddit?\n",
            "Text: \n",
            "Subreddit: AskReddit\n",
            "ID: kr8op6\n",
            "Author: Cheap_Double9726\n",
            "URL: https://www.reddit.com/r/AskReddit/comments/kr8op6/how_would_you_feel_about_a_if_you_accidentally/\n",
            "Score: 182979\n",
            "Comments: 4316\n",
            "Created: 01/05/2021, 21:47:54\n",
            "\n",
            "--- Post 7 ---\n",
            "Title: Stan Lee has passed away at 95 years old\n",
            "Text: As many of you know today is day that many of us have dreaded. Stan Lee has passed away at the age of 95. He leaves behind a legacy of superheroes and stories that have touched many people's lives for decades. We wanted to make this thread to honor and remember this wonderful man, so please use it discuss his life, his work, [his cameos](https://thumbs.gfycat.com/RapidClearDungenesscrab-small.gif), etc and what they meant to you. \n",
            "\n",
            "Excelsior!\n",
            "\n",
            "-The AskReddit mods\n",
            "Subreddit: AskReddit\n",
            "ID: 9whgf4\n",
            "Author: -eDgAR-\n",
            "URL: https://www.reddit.com/r/AskReddit/comments/9whgf4/stan_lee_has_passed_away_at_95_years_old/\n",
            "Score: 175368\n",
            "Comments: 27800\n",
            "Created: 11/12/2018, 19:54:27\n",
            "\n",
            "--- Post 8 ---\n",
            "Title: Reddit, how would you feel about a law that bans radio stations from playing commercials with honking/beeping/siren noises in them?\n",
            "Text: \n",
            "Subreddit: AskReddit\n",
            "ID: 9gx68l\n",
            "Author: san69cor\n",
            "URL: https://www.reddit.com/r/AskReddit/comments/9gx68l/reddit_how_would_you_feel_about_a_law_that_bans/\n",
            "Score: 160334\n",
            "Comments: 6746\n",
            "Created: 09/18/2018, 18:01:18\n",
            "\n",
            "--- Post 9 ---\n",
            "Title: Bill Gates said, \"I will always choose a lazy person to do a difficult job because a lazy person will find an easy way to do it.\" What's a real-life example of this?\n",
            "Text: \n",
            "Subreddit: AskReddit\n",
            "ID: himsju\n",
            "Author: lauvnoodles\n",
            "URL: https://www.reddit.com/r/AskReddit/comments/himsju/bill_gates_said_i_will_always_choose_a_lazy/\n",
            "Score: 154347\n",
            "Comments: 14817\n",
            "Created: 06/30/2020, 12:53:42\n",
            "\n",
            "--- Post 10 ---\n",
            "Title: What if Earth is like one of those uncontacted tribes in South America, like the whole Galaxy knows we're here but they've agreed not to contact us until we figure it out for ourselves?\n",
            "Text: \n",
            "Subreddit: AskReddit\n",
            "ID: kka536\n",
            "Author: slim_p_\n",
            "URL: https://www.reddit.com/r/AskReddit/comments/kka536/what_if_earth_is_like_one_of_those_uncontacted/\n",
            "Score: 152121\n",
            "Comments: 8590\n",
            "Created: 12/26/2020, 01:41:25\n",
            "\n",
            "--- Post 11 ---\n",
            "Title: Anthony Bourdain once said \"There's a guy in my head, and all he wants to do is lay in bed all day long, smoke pot, and watch old movies and cartoons. My life is a series of strategems, to avoid, and outwit that guy\". Who is \"that guy\" for you, and what do you do to avoid him?\n",
            "Text: \n",
            "Subreddit: AskReddit\n",
            "ID: dcxylq\n",
            "None\n",
            "URL: https://www.reddit.com/r/AskReddit/comments/dcxylq/anthony_bourdain_once_said_theres_a_guy_in_my/\n",
            "Score: 150823\n",
            "Comments: 11019\n",
            "Created: 10/03/2019, 22:00:30\n",
            "\n",
            "--- Post 12 ---\n",
            "Title: How is everyone enjoying Reddit while Instagram Facebook and whatsapp are all down?\n",
            "Text: \n",
            "Subreddit: AskReddit\n",
            "ID: q18zrj\n",
            "Author: PastaM0nster\n",
            "URL: https://www.reddit.com/r/AskReddit/comments/q18zrj/how_is_everyone_enjoying_reddit_while_instagram/\n",
            "Score: 148721\n",
            "Comments: 13851\n",
            "Created: 10/04/2021, 16:30:05\n",
            "\n",
            "--- Post 13 ---\n",
            "Title: It's more than likely that Covid-19 will still be around at Christmas time - how are we going to explain to kids that Santa is still allowed to go into millions of houses?\n",
            "Text: \n",
            "Subreddit: AskReddit\n",
            "ID: i4r81a\n",
            "Author: MrSpooniversal\n",
            "URL: https://www.reddit.com/r/AskReddit/comments/i4r81a/its_more_than_likely_that_covid19_will_still_be/\n",
            "Score: 145258\n",
            "Comments: 15886\n",
            "Created: 08/06/2020, 12:49:57\n",
            "\n",
            "--- Post 14 ---\n",
            "Title: Without saying what the category is, what are your top five?\n",
            "Text:  \n",
            "Subreddit: AskReddit\n",
            "ID: 99eh6b\n",
            "None\n",
            "URL: https://www.reddit.com/r/AskReddit/comments/99eh6b/without_saying_what_the_category_is_what_are_your/\n",
            "Score: 144671\n",
            "Comments: 26037\n",
            "Created: 08/22/2018, 15:50:56\n",
            "\n",
            "--- Post 15 ---\n",
            "Title: What free things online should everyone take advantage of?\n",
            "Text: \n",
            "Subreddit: AskReddit\n",
            "ID: ecscwk\n",
            "None\n",
            "URL: https://www.reddit.com/r/AskReddit/comments/ecscwk/what_free_things_online_should_everyone_take/\n",
            "Score: 141633\n",
            "Comments: 14737\n",
            "Created: 12/19/2019, 12:10:59\n",
            "\n",
            "--- Post 16 ---\n",
            "Title: With all of the negative headlines dominating the news these days, it can be difficult to spot signs of progress. What makes you optimistic about the future?\n",
            "Text: \n",
            "Subreddit: AskReddit\n",
            "ID: 80phz7\n",
            "Author: thisisbillgates\n",
            "URL: https://www.reddit.com/r/AskReddit/comments/80phz7/with_all_of_the_negative_headlines_dominating_the/\n",
            "Score: 139500\n",
            "Comments: 20968\n",
            "Created: 02/27/2018, 19:53:47\n",
            "\n",
            "--- Post 17 ---\n",
            "Title: Steve Irwin has you pinned down in a headlock, what cool facts does he tell the audience about you and your habitat?\n",
            "Text: \n",
            "Subreddit: AskReddit\n",
            "ID: ilcknh\n",
            "Author: zombiepiemaster\n",
            "URL: https://www.reddit.com/r/AskReddit/comments/ilcknh/steve_irwin_has_you_pinned_down_in_a_headlock/\n",
            "Score: 137326\n",
            "Comments: 5354\n",
            "Created: 09/02/2020, 18:35:36\n",
            "\n",
            "--- Post 18 ---\n",
            "Title: [Serious] Americans, would you be in support of putting a law in place that government officials, such as senators and the president, go without pay during shutdowns like this while other federal employees do? Why, or why not?\n",
            "Text: \n",
            "Subreddit: AskReddit\n",
            "ID: aicgpz\n",
            "Author: iamtehryan\n",
            "URL: https://www.reddit.com/r/AskReddit/comments/aicgpz/serious_americans_would_you_be_in_support_of/\n",
            "Score: 137171\n",
            "Comments: 10423\n",
            "Created: 01/21/2019, 17:38:11\n",
            "\n",
            "--- Post 19 ---\n",
            "Title: Iceland just announced that every Icelander over the age of 18 automatically become organ donors with ability to opt out. How do you feel about this?\n",
            "Text: \n",
            "Subreddit: AskReddit\n",
            "ID: ac9038\n",
            "Author: Fraktari\n",
            "URL: https://www.reddit.com/r/AskReddit/comments/ac9038/iceland_just_announced_that_every_icelander_over/\n",
            "Score: 135263\n",
            "Comments: 15279\n",
            "Created: 01/03/2019, 19:12:23\n",
            "\n",
            "--- Post 20 ---\n",
            "Title: With Christmas 364 days away, people who already have their decorations up, why?\n",
            "Text: \n",
            "Subreddit: AskReddit\n",
            "ID: kkmgzr\n",
            "Author: writer_wannabe46\n",
            "URL: https://www.reddit.com/r/AskReddit/comments/kkmgzr/with_christmas_364_days_away_people_who_already/\n",
            "Score: 133687\n",
            "Comments: 3482\n",
            "Created: 12/26/2020, 17:44:57\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1) Reconhecimento de Entidades Nomeadas"
      ],
      "metadata": {
        "id": "KdP1oskAV6Kq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Consider the following text: {question}\n",
        "\n",
        "Can you identify which named entities? List entities that are humans, entities that are animals, and entities that are neither humans nor animals.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "zOWW5NYFT5ES"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
        "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"/content/llama-2-7b-chat.Q5_0.gguf\",\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,  # Verbose is required to pass to the callback manager\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfUud9xtWXTP",
        "outputId": "3ed7aa79-185f-4318-bf2c-1312407d763f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "question = texto\n",
        "llm_chain.run(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "2A-rXZUjWLa1",
        "outputId": "3e77b6e5-e0a7-476c-caa6-17aa3efbe3f2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-c9421af0b9f8>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mllm_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLMChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mllm_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`run` supports only one positional argument.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    508\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             outputs = (\n\u001b[0;32m--> 306\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     ) -> Dict[str, str]:\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             return self.llm.generate_prompt(\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    515\u001b[0m         \u001b[0mprompt_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m                 )\n\u001b[1;32m    665\u001b[0m             ]\n\u001b[0;32m--> 666\u001b[0;31m             output = self._generate_helper(\n\u001b[0m\u001b[1;32m    667\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m         \u001b[0mflattened_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             output = (\n\u001b[0;32m--> 540\u001b[0;31m                 self._generate(\n\u001b[0m\u001b[1;32m    541\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m             text = (\n\u001b[0;32m-> 1069\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/llms/llamacpp.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;31m# and return the combined strings from the first choices's text:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mcombined_text_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             for chunk in self._stream(\n\u001b[0m\u001b[1;32m    292\u001b[0m                 \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/llms/llamacpp.py\u001b[0m in \u001b[0;36m_stream\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m             \u001b[0mlogprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"logprobs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             chunk = GenerationChunk(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36m_create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_tokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1422\u001b[0m                 \u001b[0;34mf\"Requested tokens ({len(prompt_tokens)}) exceed context window of {llama_cpp.llama_n_ctx(self.ctx)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Requested tokens (3417) exceed context window of 512"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Como vemos acima, há uma limitação no número de tokens da entrada. Por conta disso, listarei menos posts e colocarei apenas as perguntas dos usuários, sem toda a formatação que podemos ver lá em cima.**"
      ],
      "metadata": {
        "id": "bwWwfHkzX7dV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_posts = subreddit.top(limit=10)\n",
        "print(top_posts, \"\\n\")\n",
        "\n",
        "cont = 1\n",
        "\n",
        "texto = ''\n",
        "for post in top_posts:\n",
        "    texto += 'Question ' + str(cont) + ': ' + post.title + '\\n\\n'\n",
        "    cont += 1\n",
        "\n",
        "print(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jatj0kaDXyHw",
        "outputId": "4f1bf42d-4e5b-45db-f84d-395d3ce92e63"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<praw.models.listing.generator.ListingGenerator object at 0x789db4d558a0> \n",
            "\n",
            "Question 1: People who haven't pooped in 2019 yet, why are you still holding on to last years shit?\n",
            "\n",
            "Question 2: How would you feel about Reddit adding 3 NSFW filters to distinguish between porn, gore, and repetitive posts asking how you would feel about Reddit adding 2 NSFW filters to distinguish between porn and gore?\n",
            "\n",
            "Question 3: Would you watch a show where a billionaire CEO has to go an entire month on their lowest paid employees salary, without access to any other resources than that of the employee? What do you think would happen?\n",
            "\n",
            "Question 4: What if God came down one day and said \"It's pronounced 'Jod' then left?\n",
            "\n",
            "Question 5: How would you feel about a feature where if someone upvotes a crosspost, the original post is upvoted automatically?\n",
            "\n",
            "Question 6: How would you feel about a \"if you accidentally scroll to the top, you can go back to where you were,\" button for Reddit?\n",
            "\n",
            "Question 7: Stan Lee has passed away at 95 years old\n",
            "\n",
            "Question 8: Reddit, how would you feel about a law that bans radio stations from playing commercials with honking/beeping/siren noises in them?\n",
            "\n",
            "Question 9: Bill Gates said, \"I will always choose a lazy person to do a difficult job because a lazy person will find an easy way to do it.\" What's a real-life example of this?\n",
            "\n",
            "Question 10: What if Earth is like one of those uncontacted tribes in South America, like the whole Galaxy knows we're here but they've agreed not to contact us until we figure it out for ourselves?\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "question = texto\n",
        "llm_chain.run(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ozUZP-RnYpCD",
        "outputId": "c801dbd0-0642-4ea3-af13-da4f5cc4c59c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos ver, o modelo não respondeu a um prompt desse tamanho.\n",
        "\n",
        "(Eu estou deixando os erros aqui pois é o único material que tenho para aprensentar, como se fosse um relatório em tempo real do meu desenvolvimento.)\n",
        "\n",
        "Vou separar por apenas uma pergunta para ver se o modelo consegue entender."
      ],
      "metadata": {
        "id": "JEjBt78KZcLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_posts = subreddit.top(limit=1)\n",
        "print(top_posts, \"\\n\")\n",
        "\n",
        "cont = 1\n",
        "\n",
        "texto = ''\n",
        "for post in top_posts:\n",
        "    texto += 'Question ' + str(cont) + ': ' + post.title + '\\n\\n'\n",
        "    cont += 1\n",
        "\n",
        "print(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvkFoGrvYtUV",
        "outputId": "9f88ef84-801d-456e-d8c3-3a3aa0e60e9a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<praw.models.listing.generator.ListingGenerator object at 0x789db4d57c10> \n",
            "\n",
            "Question 1: People who haven't pooped in 2019 yet, why are you still holding on to last years shit?\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = texto\n",
        "\n",
        "template = \"\"\"Consider the following text: {question}\n",
        "\n",
        "List the named entities. List the named entities that are humans. List the named entities that are dates. List the named entities that are animals.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "o7FpwgbuaNo9"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "llm_chain.run(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "Iw9r1RAzZ6Qa",
        "outputId": "452d22e2-fdf3-40c6-fa4f-53990fa73c2a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Answer:\n",
            "Named Entities:\n",
            "\n",
            "Humans: People, 2019\n",
            "Animals: None mentioned\n",
            "Dates: 2019\n",
            "\n",
            "Explanation:\n",
            "In this text, the following named entities can be identified:\n",
            "\n",
            "* Humans: \"People\" is mentioned once, referring to individuals who have not pooped in 2019.\n",
            "* Dates: \"2019\" is mentioned twice, once as a reference to the current year and once in the context of holding onto last year's feces.\n",
            "* Animals: None are mentioned in the text."
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\n\\n\\n\\n\\nAnswer:\\nNamed Entities:\\n\\nHumans: People, 2019\\nAnimals: None mentioned\\nDates: 2019\\n\\nExplanation:\\nIn this text, the following named entities can be identified:\\n\\n* Humans: \"People\" is mentioned once, referring to individuals who have not pooped in 2019.\\n* Dates: \"2019\" is mentioned twice, once as a reference to the current year and once in the context of holding onto last year\\'s feces.\\n* Animals: None are mentioned in the text.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note que apesar da pergunta ser de um tema bem infame (desculpe por isso, mas foi aleatório), o modelo conseguiu identificar humanos e datas, mesmo tendo incluído 2019 como um humano na primeira listagem. Além disso, ele errou uma coisa interessante: ele entendeu que implicitamente estava sendo citado 2019 mais uma vez (em \"last years\"), o que, apesar de errado (pois o ano anterior era 2018, obviamente), é curioso que ele tenha entendido que uma outra \"entidade de data\" estava sendo referenciada."
      ],
      "metadata": {
        "id": "gwEmeCEJbdgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora, vou criar um vetor com os 20 primeiros posts e testar como o modelo se comporta com eles separadamente."
      ],
      "metadata": {
        "id": "ZkjZLN5rcJ2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_posts = subreddit.top(limit=20)\n",
        "print(top_posts, \"\\n\")\n",
        "\n",
        "cont = 1\n",
        "\n",
        "textos = []\n",
        "texto = ''\n",
        "for post in top_posts:\n",
        "    texto = 'Question ' + str(cont) + ': ' + post.title + '\\n\\n'\n",
        "    textos.append(texto)\n",
        "    cont += 1\n",
        "\n",
        "print(textos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnlcgKIwcIPo",
        "outputId": "c1fa3bb0-f4c1-4d23-9da8-07971d6aceec"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<praw.models.listing.generator.ListingGenerator object at 0x789c7ab5f430> \n",
            "\n",
            "[\"Question 1: People who haven't pooped in 2019 yet, why are you still holding on to last years shit?\\n\\n\", 'Question 2: How would you feel about Reddit adding 3 NSFW filters to distinguish between porn, gore, and repetitive posts asking how you would feel about Reddit adding 2 NSFW filters to distinguish between porn and gore?\\n\\n', 'Question 3: Would you watch a show where a billionaire CEO has to go an entire month on their lowest paid employees salary, without access to any other resources than that of the employee? What do you think would happen?\\n\\n', 'Question 4: What if God came down one day and said \"It\\'s pronounced \\'Jod\\' then left?\\n\\n', 'Question 5: How would you feel about a feature where if someone upvotes a crosspost, the original post is upvoted automatically?\\n\\n', 'Question 6: How would you feel about a \"if you accidentally scroll to the top, you can go back to where you were,\" button for Reddit?\\n\\n', 'Question 7: Stan Lee has passed away at 95 years old\\n\\n', 'Question 8: Reddit, how would you feel about a law that bans radio stations from playing commercials with honking/beeping/siren noises in them?\\n\\n', 'Question 9: Bill Gates said, \"I will always choose a lazy person to do a difficult job because a lazy person will find an easy way to do it.\" What\\'s a real-life example of this?\\n\\n', \"Question 10: What if Earth is like one of those uncontacted tribes in South America, like the whole Galaxy knows we're here but they've agreed not to contact us until we figure it out for ourselves?\\n\\n\", 'Question 11: Anthony Bourdain once said \"There\\'s a guy in my head, and all he wants to do is lay in bed all day long, smoke pot, and watch old movies and cartoons. My life is a series of strategems, to avoid, and outwit that guy\". Who is \"that guy\" for you, and what do you do to avoid him?\\n\\n', 'Question 12: How is everyone enjoying Reddit while Instagram Facebook and whatsapp are all down?\\n\\n', \"Question 13: It's more than likely that Covid-19 will still be around at Christmas time - how are we going to explain to kids that Santa is still allowed to go into millions of houses?\\n\\n\", 'Question 14: Without saying what the category is, what are your top five?\\n\\n', 'Question 15: What free things online should everyone take advantage of?\\n\\n', 'Question 16: With all of the negative headlines dominating the news these days, it can be difficult to spot signs of progress. What makes you optimistic about the future?\\n\\n', 'Question 17: Steve Irwin has you pinned down in a headlock, what cool facts does he tell the audience about you and your habitat?\\n\\n', 'Question 18: [Serious] Americans, would you be in support of putting a law in place that government officials, such as senators and the president, go without pay during shutdowns like this while other federal employees do? Why, or why not?\\n\\n', 'Question 19: Iceland just announced that every Icelander over the age of 18 automatically become organ donors with ability to opt out. How do you feel about this?\\n\\n', 'Question 20: With Christmas 364 days away, people who already have their decorations up, why?\\n\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for texto in textos:\n",
        "    print(texto)\n",
        "    question = texto\n",
        "    template = \"\"\"Consider the following text: {question}\n",
        "    List the named entities. List the named entities that are humans. List the named entities that are dates. List the named entities that are animals.\"\"\"\n",
        "    prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "    llm_chain.run(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iW8_Wqhkces8",
        "outputId": "88fe4b88-8ac2-4fb7-e1ac-1b951f599fec"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 1: People who haven't pooped in 2019 yet, why are you still holding on to last years shit?\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Answer:\n",
            "Named Entities:\n",
            "\n",
            "Humans:\n",
            "\n",
            "* People (mentioned in the context of not pooping)\n",
            "\n",
            "Dates:\n",
            "\n",
            "* 2019 (mentioned as the current year)\n",
            "\n",
            "Animals:\n",
            "\n",
            "* No animals are mentioned in the text.Question 2: How would you feel about Reddit adding 3 NSFW filters to distinguish between porn, gore, and repetitive posts asking how you would feel about Reddit adding 2 NSFW filters to distinguish between porn and gore?\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "I have identified the following named entities in the text:\n",
            "\n",
            "Humans:\n",
            "\n",
            "* Reddit (organization)\n",
            "* you (human user)\n",
            "\n",
            "Dates:\n",
            "\n",
            "* none\n",
            "\n",
            "Animals:\n",
            "\n",
            "* none\n",
            "\n",
            "So, the named entities in the text are:\n",
            "\n",
            "Humans: 2 (Reddit, you)\n",
            "Dates: 0\n",
            "Animals: 0Question 3: Would you watch a show where a billionaire CEO has to go an entire month on their lowest paid employees salary, without access to any other resources than that of the employee? What do you think would happen?\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Answer: Named Entities:\n",
            "\n",
            "1. Billionaire CEO\n",
            "2. lowest paid employees\n",
            "3. month\n",
            "4. resources\n",
            "5. employee\n",
            "\n",
            "Named Entities (Humans):\n",
            "\n",
            "1. none mentioned\n",
            "\n",
            "Named Entities (Dates):\n",
            "\n",
            "1. none mentioned\n",
            "\n",
            "Named Entities (Animals):\n",
            "\n",
            "1. none mentionedQuestion 4: What if God came down one day and said \"It's pronounced 'Jod' then left?\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Answer:\n",
            "\n",
            "Named Entities (Humans):\n",
            "\n",
            "* God\n",
            "\n",
            "Named Entities (Dates):\n",
            "\n",
            "* one day\n",
            "\n",
            "Named Entities (Animals):\n",
            "\n",
            "* left\n",
            "\n",
            "Note: The text does not mention any other animals, so there are no named entities in this category.Question 5: How would you feel about a feature where if someone upvotes a crosspost, the original post is upvoted automatically?\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Answer: Named Entities (Humans):\n",
            "\n",
            "* You\n",
            "* someone\n",
            "\n",
            "Named Entities (Dates):\n",
            "\n",
            "* automatically\n",
            "\n",
            "Named Entities (Animals):\n",
            "\n",
            "* none mentionedQuestion 6: How would you feel about a \"if you accidentally scroll to the top, you can go back to where you were,\" button for Reddit?\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "My answer:\n",
            "\n",
            "Named Entities: Humans\n",
            "\n",
            "* You (plural)\n",
            "* Redditor (singular)\n",
            "\n",
            "Named Entities: Dates\n",
            "\n",
            "* Top (singular)\n",
            "* bottom (singular)\n",
            "\n",
            "Named Entities: Animals\n",
            "\n",
            "* none mentioned in the text\n",
            "\n",
            "Explanation: In this text, there are three types of named entities that can be identified:\n",
            "* Humans: The text mentions \"you\" and \"Redditor\", which are both referring to humans.\n",
            "* Dates: The text mentions \"top\" and \"bottom\", which could refer to any date or time period, but in this context seem to be used as a metaphor for scrolling up and down on Reddit.\n",
            "* Animals: None of the named entities in the text refer to animals. Therefore, there are no named entities that belong to this category.Question 7: Stan Lee has passed away at 95 years old\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Answer:\n",
            "\n",
            "Named Entities (Humans):\n",
            "\n",
            "1. Stan Lee\n",
            "2. years old\n",
            "\n",
            "Named Entities (Dates):\n",
            "\n",
            "1. 95 years old\n",
            "\n",
            "Named Entities (Animals):\n",
            "\n",
            "0 (no named entities)\n",
            "\n",
            "Explanation:\n",
            "\n",
            "* Named Entities (Humans): Stan Lee is a human being mentioned in the text.\n",
            "* Named Entities (Dates): The date of Stan Lee's passing is mentioned in the text, which is 95 years old.\n",
            "* Named Entities (Animals): None of the named entities in the text are animals.Question 8: Reddit, how would you feel about a law that bans radio stations from playing commercials with honking/beeping/siren noises in them?\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " List the named entities that are locations.\n",
            "Please provide your answers to these questions:\n",
            "1. What are the named entities in the text?\n",
            "2. How many human named entities are there in the text?\n",
            "3. What is the date mentioned in the text?\n",
            "4. Are there any animal named entities in the text? If so, list them.\n",
            "5. Are there any location named entities in the text? If so, list them.\n",
            "6. Can you explain why a law banning radio stations from playing commercials with honking/beeping/siren noises would be beneficial or harmful to society?Question 9: Bill Gates said, \"I will always choose a lazy person to do a difficult job because a lazy person will find an easy way to do it.\" What's a real-life example of this?\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Answer:\n",
            "\n",
            "Named Entities (Humans):\n",
            "\n",
            "* Bill Gates\n",
            "* lazy person\n",
            "\n",
            "Named Entities (Dates):\n",
            "\n",
            "* None\n",
            "\n",
            "Named Entities (Animals):\n",
            "\n",
            "* NoneQuestion 10: What if Earth is like one of those uncontacted tribes in South America, like the whole Galaxy knows we're here but they've agreed not to contact us until we figure it out for ourselves?\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "I will provide the answers and explain why I think the author uses these techniques.\n",
            "\n",
            "Answer: Named Entities:\n",
            "\n",
            "* Earth\n",
            "* Galaxy (not a human entity)\n",
            "* uncontacted tribes (plural, not a single entity)\n",
            "* South America (geographical location)\n",
            "* Tribes (plural, not a single entity)\n",
            "\n",
            "Named Humans:\n",
            "\n",
            "* None listed.\n",
            "\n",
            "Named Dates:\n",
            "\n",
            "* None listed.\n",
            "\n",
            "Named Animals:\n",
            "\n",
            "* None listed.\n",
            "\n",
            "Explanation:\n",
            "The author uses these techniques to create a thought-provoking and imaginative scenario. By comparing Earth to an uncontacted tribe, the author highlights the idea that Earth is a complex and mysterious place that may be beyond the understanding of even the most advanced civilizations in the galaxy. The use of plural nouns for \"uncontacted tribes\" and \"tribes\" emphasizes the idea that there are many such groups scattered across the planet, each with their own unique cultures and ways of life. By not including any named entities or dates, the author focuses the reader's attention on the central idea of the passage ratherQuestion 11: Anthony Bourdain once said \"There's a guy in my head, and all he wants to do is lay in bed all day long, smoke pot, and watch old movies and cartoons. My life is a series of strategems, to avoid, and outwit that guy\". Who is \"that guy\" for you, and what do you do to avoid him?\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  \n",
            "Answer: \n",
            "Humans: \n",
            "- Anthony Bourdain\n",
            "- that guy\n",
            "\n",
            "Dates: \n",
            "- none mentioned\n",
            "\n",
            "Animals: \n",
            "- none mentionedQuestion 12: How is everyone enjoying Reddit while Instagram Facebook and whatsapp are all down?\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Answer: Named Entities:\n",
            "\n",
            "Humans:\n",
            "\n",
            "* Everyone (1 instance"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-cc1d93d420af>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPromptTemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mllm_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLMChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mllm_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`run` supports only one positional argument.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    508\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             outputs = (\n\u001b[0;32m--> 306\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     ) -> Dict[str, str]:\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             return self.llm.generate_prompt(\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    515\u001b[0m         \u001b[0mprompt_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m                 )\n\u001b[1;32m    665\u001b[0m             ]\n\u001b[0;32m--> 666\u001b[0;31m             output = self._generate_helper(\n\u001b[0m\u001b[1;32m    667\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m         \u001b[0mflattened_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             output = (\n\u001b[0;32m--> 540\u001b[0;31m                 self._generate(\n\u001b[0m\u001b[1;32m    541\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m             text = (\n\u001b[0;32m-> 1069\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/llms/llamacpp.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;31m# and return the combined strings from the first choices's text:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mcombined_text_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             for chunk in self._stream(\n\u001b[0m\u001b[1;32m    292\u001b[0m                 \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/llms/llamacpp.py\u001b[0m in \u001b[0;36m_stream\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m             \u001b[0mlogprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"logprobs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             chunk = GenerationChunk(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36m_create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1466\u001b[0m         \u001b[0mfinish_reason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"length\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m         \u001b[0mmultibyte_fix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1468\u001b[0;31m         for token in self.generate(\n\u001b[0m\u001b[1;32m   1469\u001b[0m             \u001b[0mprompt_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1470\u001b[0m             \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m             token = self.sample(\n\u001b[1;32m   1245\u001b[0m                 \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m   1062\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_past\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             )\n\u001b[0;32m-> 1064\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m             \u001b[0;31m# Save tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_past\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mn_past\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_tokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         return_code = llama_cpp.llama_decode(\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama_cpp.py\u001b[0m in \u001b[0;36mllama_decode\u001b[0;34m(ctx, batch)\u001b[0m\n\u001b[1;32m   1470\u001b[0m     \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcould\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfind\u001b[0m \u001b[0ma\u001b[0m \u001b[0mKV\u001b[0m \u001b[0mslot\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mtry\u001b[0m \u001b[0mreducing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msize\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mincrease\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m     < 0 - error\"\"\"\n\u001b[0;32m-> 1472\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllama_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A execução da célula acima foi interrompida por mim após 11 questões, por conta da demora - quase meia hora, nesse caso - do modelo para responder (que foi explicada no começo do relatório).**\n",
        "\n",
        "Fato é que para essa tarefa o modelo se saiu bem, apesar da demora no processamento, que não é problema dele, mas do ambiente.\n",
        "\n",
        "**Por conta da lentidão do ambiente, a partir daqui irei usar exemplos simples, mas que podem claramente se estender para corpus maiores."
      ],
      "metadata": {
        "id": "uHtpahrsi2f7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **(2) Extração de palavras-chave**"
      ],
      "metadata": {
        "id": "udhbg8WbmsJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui, tentarei extrair as principais palavras de cada um dos posts."
      ],
      "metadata": {
        "id": "3_duxf4hnpyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_posts = subreddit.top(limit=10)\n",
        "print(top_posts, \"\\n\")\n",
        "\n",
        "cont = 1\n",
        "\n",
        "textos = []\n",
        "texto = ''\n",
        "for post in top_posts:\n",
        "    texto = 'Question ' + str(cont) + ': ' + post.title + '\\n\\n'\n",
        "    textos.append(texto)\n",
        "    cont += 1\n",
        "\n",
        "print(textos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOBplnuemy3x",
        "outputId": "dd6b907d-24ef-444d-abb6-9b8ecfa566c0"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<praw.models.listing.generator.ListingGenerator object at 0x789c7ab5d450> \n",
            "\n",
            "[\"Question 1: People who haven't pooped in 2019 yet, why are you still holding on to last years shit?\\n\\n\", 'Question 2: How would you feel about Reddit adding 3 NSFW filters to distinguish between porn, gore, and repetitive posts asking how you would feel about Reddit adding 2 NSFW filters to distinguish between porn and gore?\\n\\n', 'Question 3: Would you watch a show where a billionaire CEO has to go an entire month on their lowest paid employees salary, without access to any other resources than that of the employee? What do you think would happen?\\n\\n', 'Question 4: What if God came down one day and said \"It\\'s pronounced \\'Jod\\' then left?\\n\\n', 'Question 5: How would you feel about a feature where if someone upvotes a crosspost, the original post is upvoted automatically?\\n\\n', 'Question 6: How would you feel about a \"if you accidentally scroll to the top, you can go back to where you were,\" button for Reddit?\\n\\n', 'Question 7: Stan Lee has passed away at 95 years old\\n\\n', 'Question 8: Reddit, how would you feel about a law that bans radio stations from playing commercials with honking/beeping/siren noises in them?\\n\\n', 'Question 9: Bill Gates said, \"I will always choose a lazy person to do a difficult job because a lazy person will find an easy way to do it.\" What\\'s a real-life example of this?\\n\\n', \"Question 10: What if Earth is like one of those uncontacted tribes in South America, like the whole Galaxy knows we're here but they've agreed not to contact us until we figure it out for ourselves?\\n\\n\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "post3 = textos[2]\n",
        "post8 = textos[7]\n",
        "\n",
        "print(post3)\n",
        "print(post8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AabMs9g2nmKd",
        "outputId": "920fbdb4-c6ae-4ba9-8341-e5c064fa5da6"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 3: Would you watch a show where a billionaire CEO has to go an entire month on their lowest paid employees salary, without access to any other resources than that of the employee? What do you think would happen?\n",
            "\n",
            "\n",
            "Question 8: Reddit, how would you feel about a law that bans radio stations from playing commercials with honking/beeping/siren noises in them?\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(post3)\n",
        "question = post3\n",
        "template = \"\"\"Consider the following text: {question}\n",
        "\n",
        "List the keywords in the text. What is the main word in the text?.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "llm_chain.run(question)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z03zZmNlpDMa",
        "outputId": "ba1fa6b8-5d7f-473d-931e-24a56f23d38d"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 3: Would you watch a show where a billionaire CEO has to go an entire month on their lowest paid employees salary, without access to any other resources than that of the employee? What do you think would happen?\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " How many times does each keyword appear in the text?\n",
            "\n",
            "\n",
            "Answer:\n",
            "Keywords in the Text:\n",
            "\n",
            "1. Show - appears once\n",
            "2. watch - appears once\n",
            "3. billionaire - appears once\n",
            "4. CEO - appears once\n",
            "5. month - appears twice\n",
            "6. salary - appears three times\n",
            "7. employee - appears three times\n",
            "8. resources - appears once\n",
            "9. happen - appears once\n",
            "\n",
            "Main Word in the Text:\n",
            "The main word in the text is \"month\". It appears twice in the text.\n",
            "Frequency of Each Keyword in the Text:\n",
            "1. Show - 1 time\n",
            "2. watch - 1 time\n",
            "3. billionaire - 1 time\n",
            "4. CEO - 1 time\n",
            "5. month - 2 times\n",
            "6. salary - 3 times\n",
            "7. employee - 3 times\n",
            "8. resources - 1 time\n",
            "9. happen - 1 time\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(post8)\n",
        "question = post8\n",
        "template = \"\"\"Consider the following text: {question}\n",
        "\n",
        "List the keywords in the text. What is the main word in the text?.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "llm_chain.run(question)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3r04etfrjzW",
        "outputId": "c67cc409-2ba5-41e4-8063-74827fe03a1f"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 8: Reddit, how would you feel about a law that bans radio stations from playing commercials with honking/beeping/siren noises in them?\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " How many times does each keyword appear in the text?\n",
            "\n",
            "\n",
            "\n",
            "Keywords:\n",
            "Radio, stations, playing, commercials, bans, noises, honking, beeping, siren \n",
            "\n",
            "Main word: Law\n",
            "Appearance of keywords:\n",
            "Radio - 3 times\n",
            "stations - 2 times\n",
            "playing - 2 times\n",
            "commercials - 2 times\n",
            "bans - 1 time\n",
            "noises - 2 times\n",
            "honking - 1 time\n",
            "beeping - 1 time\n",
            "siren - 1 time\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note que para a tarefa de extração de palavras-chave, o modelo fez mais do que eu pedi no prompt. Esse ponto eu não consegui ajustar. Ficou me parecendo que ele faz um match do meu prompt com tarefas que ele já conhece e é capaz de executar, e então executa as já conhecidas, mesmo que a minha seja mais simples."
      ],
      "metadata": {
        "id": "czW-L1ecsei7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iLod6rBIrli8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}